{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43136a12",
   "metadata": {},
   "source": [
    "# **Bài thực hành: Biểu diễn Chuỗi cho Mạng Nơ-ron Hồi quy (RNNs)**\n",
    "\n",
    "**Họ và tên:** Huỳnh Thế Hy\n",
    "**Mã số sinh viên:** 051205009083\n",
    "\n",
    "## **1. Các Kỹ thuật Mã hóa Số học (Numerical Encoding)**\n",
    "\n",
    "Chuyển đổi các phần tử (token) trong chuỗi (ví dụ: từ, ký tự) thành các giá trị số.\n",
    "\n",
    "### **1.1. Mã hóa số nguyên (Integer Encoding)**\n",
    "\n",
    "Phương pháp này gán một số nguyên duy nhất cho mỗi token riêng biệt trong từ điển (vocabulary).\n",
    "\n",
    "**Giải thích:**\n",
    "- **Bước 1: Xây dựng từ điển:** Duyệt qua toàn bộ tập dữ liệu để tìm ra tất cả các token duy nhất (ví dụ: các từ).\n",
    "- **Bước 2: Gán ID:** Tạo hai cấu trúc dữ liệu: một map từ token sang số nguyên (ID) và một map ngược lại từ ID sang token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57806e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Vocabulary ---\n",
      "{'an': 1, 'artificial': 2, 'deep': 3, 'developing': 4, 'exciting': 5, 'fast': 6, 'field': 7, 'future': 8, 'intelligence': 9, 'is': 10, 'learning': 11, 'the': 12, 'very': 13}\n",
      "\n",
      "--- Original Sentence ---\n",
      "deep learning is an exciting field\n",
      "\n",
      "--- Câu đã được mã hóa số nguyên ---\n",
      "[3, 11, 10, 1, 5, 7]\n"
     ]
    }
   ],
   "source": [
    "# Dữ liệu văn bản mẫu bằng tiếng Anh\n",
    "corpus = [\n",
    "    \"deep learning is an exciting field\",\n",
    "    \"deep learning is developing very fast\",\n",
    "    \"artificial intelligence is the future\"\n",
    "]\n",
    "\n",
    "# Tách các câu thành từng từ\n",
    "words = set()\n",
    "for sentence in corpus:\n",
    "    for word in sentence.split(' '):\n",
    "        words.add(word)\n",
    "\n",
    "# Sắp xếp để đảm bảo thứ tự nhất quán\n",
    "words = sorted(list(words))\n",
    "\n",
    "# Xây dựng từ điển: map từ sang số nguyên và ngược lại\n",
    "word_to_int = {word: i+1 for i, word in enumerate(words)} # Bắt đầu từ 1, 0 dành cho padding\n",
    "int_to_word = {i+1: word for i, word in enumerate(words)}\n",
    "\n",
    "print(\"--- Vocabulary ---\")\n",
    "print(word_to_int)\n",
    "print(\"\\n--- Original Sentence ---\")\n",
    "print(corpus[0])\n",
    "\n",
    "# Mã hóa câu đầu tiên thành chuỗi số nguyên\n",
    "integer_encoded = [word_to_int[word] for word in corpus[0].split(' ')]\n",
    "print(\"\\n--- Câu đã được mã hóa số nguyên ---\")\n",
    "print(integer_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abddbbf",
   "metadata": {},
   "source": [
    "### **1.2. Mã hóa One-Hot (One-Hot Encoding)**\n",
    "\n",
    "Từ các chỉ số nguyên đã tạo, phương pháp này biểu diễn mỗi token bằng một vector nhị phân. Vector này có độ dài bằng kích thước của từ điển, chứa toàn số 0 ngoại trừ vị trí tương ứng với chỉ số của token đó sẽ là 1.\n",
    "\n",
    "**Giải thích:**\n",
    "- **Ưu điểm:** Biểu diễn rõ ràng, không tạo ra mối quan hệ thứ tự giả tạo giữa các từ.\n",
    "- **Nhược điểm:** Tạo ra các vector rất lớn và thưa thớt (sparse) khi từ điển có kích thước lớn, dẫn đến tốn kém bộ nhớ và tính toán.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9a94fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước từ điển: 14\n",
      "Chuỗi số nguyên: tensor([ 3, 11, 10,  1,  5,  7])\n",
      "\n",
      "--- Vector One-Hot cho câu đầu tiên ---\n",
      "Kích thước của tensor đầu ra: torch.Size([6, 14])\n",
      "Vector One-Hot (chỉ hiển thị 3 từ đầu tiên):\n",
      "tensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Lấy kích thước từ điển\n",
    "vocab_size = len(word_to_int) + 1 # +1 vì chúng ta bắt đầu từ 1\n",
    "\n",
    "# Câu đã được mã hóa số nguyên từ ví dụ trước\n",
    "integer_encoded_tensor = torch.tensor(integer_encoded)\n",
    "\n",
    "print(f\"Kích thước từ điển: {vocab_size}\")\n",
    "print(f\"Chuỗi số nguyên: {integer_encoded_tensor}\")\n",
    "\n",
    "# Sử dụng one_hot của PyTorch\n",
    "one_hot_encoded = F.one_hot(integer_encoded_tensor, num_classes=vocab_size)\n",
    "\n",
    "print(\"\\n--- Vector One-Hot cho câu đầu tiên ---\")\n",
    "print(f\"Kích thước của tensor đầu ra: {one_hot_encoded.shape}\") # (số từ, kích thước từ điển)\n",
    "print(\"Vector One-Hot (chỉ hiển thị 3 từ đầu tiên):\")\n",
    "print(one_hot_encoded[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a4ae66",
   "metadata": {},
   "source": [
    "### **1.3. Nhúng (Embeddings)**\n",
    "\n",
    "Đây là phương pháp hiệu quả và phổ biến nhất hiện nay, đặc biệt với dữ liệu có số lượng lớn các hạng mục như từ vựng trong ngôn ngữ. Embeddings học cách biểu diễn mỗi token bằng một vector số thực, dày đặc (dense) và có số chiều thấp.\n",
    "\n",
    "**Giải thích:**\n",
    "- **Ưu điểm:**\n",
    "    - **Tiết kiệm không gian:** Vector có số chiều nhỏ hơn nhiều so với one-hot encoding.\n",
    "    - **Nắm bắt ngữ nghĩa:** Các từ có ngữ nghĩa tương tự nhau sẽ có vector biểu diễn gần nhau trong không gian vector. Mối quan hệ này được mô hình tự học trong quá trình huấn luyện.\n",
    "- **Cách hoạt động trong PyTorch:** Lớp `torch.nn.Embedding` hoạt động như một bảng tra cứu (lookup table). Nó lưu trữ các vector embedding cho toàn bộ từ điển. Khi nhận đầu vào là một chuỗi các chỉ số nguyên, nó sẽ trả về chuỗi các vector embedding tương ứng.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581091ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Kích thước đầu vào (chuỗi số nguyên): torch.Size([6]) ---\n",
      "tensor([ 3, 11, 10,  1,  5,  7])\n",
      "\n",
      "--- Kích thước đầu ra (chuỗi vector embedding): torch.Size([6, 10]) ---\n",
      "(Mỗi từ giờ được biểu diễn bằng một vector 10 chiều)\n",
      "[[ 2.2684345  -0.8420318  -0.2562283  -0.47288463  0.11501715 -1.2211939\n",
      "  -0.0320097  -1.917949   -0.6661722   0.42563966]\n",
      " [-0.23425567  0.12202303 -0.40725517  0.7701454  -0.88705724  1.0224806\n",
      "  -0.39175466 -0.30833176  1.6936749  -0.68035895]\n",
      " [ 0.08274392  0.13388506 -1.2006048   0.5234048  -1.0057883  -1.5691855\n",
      "   1.1583378   0.35246137  0.40919307 -1.3963822 ]\n",
      " [-0.5280668  -1.308055   -1.1671493   0.9971711   1.1483319   0.46734297\n",
      "  -0.6800098  -0.2478342  -0.5165013   0.3727347 ]\n",
      " [-0.47804928  0.3742908  -0.49524206  0.85577697  0.6889175   2.0217986\n",
      "  -1.5589483   0.18600263  0.13594964 -0.51672846]\n",
      " [-1.1509833  -1.4651959  -0.09104341 -0.28671226  1.2390982   1.7292491\n",
      "  -0.00551405  0.33005065 -0.56327575 -1.2585237 ]]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Các tham số\n",
    "vocab_size = len(word_to_int) + 1  # Kích thước từ điển\n",
    "embedding_dim = 10                 # Số chiều của vector embedding (tùy chọn)\n",
    "\n",
    "# Khởi tạo lớp Embedding\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "# Chuỗi số nguyên đầu vào\n",
    "input_tensor = torch.tensor(integer_encoded, dtype=torch.long)\n",
    "\n",
    "# Lấy vector embedding\n",
    "word_embeddings = embedding_layer(input_tensor)\n",
    "\n",
    "print(f\"--- Kích thước đầu vào (chuỗi số nguyên): {input_tensor.shape} ---\")\n",
    "print(input_tensor)\n",
    "print(f\"\\n--- Kích thước đầu ra (chuỗi vector embedding): {word_embeddings.shape} ---\")\n",
    "print(\"(Mỗi từ giờ được biểu diễn bằng một vector 10 chiều)\")\n",
    "print(word_embeddings.detach().numpy()) # detach() để không tính gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8191daa7",
   "metadata": {},
   "source": [
    "## **2. Xử lý Chuỗi có Độ dài Thay đổi (Handling Variable Lengths)**\n",
    "\n",
    "Khi xử lý dữ liệu theo lô (batch), các mô hình deep learning yêu cầu các chuỗi trong cùng một lô phải có cùng độ dài. Tuy nhiên, trong thực tế, các câu văn thường có độ dài khác nhau. Do đó, chúng ta cần hai kỹ thuật chính: **Padding** và **Truncation**.\n",
    "\n",
    "### **2.1. Padding**\n",
    "\n",
    "Padding là kỹ thuật thêm các token đặc biệt (thường là số 0) vào cuối các chuỗi ngắn hơn để làm cho chúng có độ dài bằng với chuỗi dài nhất trong lô."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bad94d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Các chuỗi trước khi padding ---\n",
      "Độ dài=6: [3, 11, 10, 1, 5, 7]\n",
      "Độ dài=6: [3, 11, 10, 4, 13, 6]\n",
      "Độ dài=5: [2, 9, 10, 12, 8]\n",
      "\n",
      "--- Các chuỗi sau khi padding (đến độ dài 6) ---\n",
      "Độ dài=6: [3, 11, 10, 1, 5, 7]\n",
      "Độ dài=6: [3, 11, 10, 4, 13, 6]\n",
      "Độ dài=6: [2, 9, 10, 12, 8, 0]\n",
      "\n",
      "--- Tensor cuối cùng ---\n",
      "tensor([[ 3, 11, 10,  1,  5,  7],\n",
      "        [ 3, 11, 10,  4, 13,  6],\n",
      "        [ 2,  9, 10, 12,  8,  0]])\n"
     ]
    }
   ],
   "source": [
    "# Mã hóa toàn bộ corpus\n",
    "sequences = [[word_to_int[word] for word in s.split()] for s in corpus]\n",
    "print(\"--- Các chuỗi trước khi padding ---\")\n",
    "for seq in sequences:\n",
    "    print(f\"Độ dài={len(seq)}: {seq}\")\n",
    "\n",
    "# Tìm độ dài của chuỗi dài nhất\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "# Thực hiện padding\n",
    "padded_sequences = []\n",
    "for seq in sequences:\n",
    "    # Lấy độ dài cần pad\n",
    "    pad_length = max_length - len(seq)\n",
    "    # Tạo tensor chứa các số 0 và nối vào cuối chuỗi\n",
    "    padded_seq = seq + [0] * pad_length\n",
    "    padded_sequences.append(padded_seq)\n",
    "\n",
    "print(f\"\\n--- Các chuỗi sau khi padding (đến độ dài {max_length}) ---\")\n",
    "for seq in padded_sequences:\n",
    "    print(f\"Độ dài={len(seq)}: {seq}\")\n",
    "\n",
    "# Chuyển thành tensor\n",
    "padded_tensor = torch.tensor(padded_sequences)\n",
    "print(\"\\n--- Tensor cuối cùng ---\")\n",
    "print(padded_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9a9b0c",
   "metadata": {},
   "source": [
    "### **2.2. Cắt ngắn (Truncation)**\n",
    "\n",
    "Ngược lại với padding, truncation là kỹ thuật cắt bớt các chuỗi dài hơn một ngưỡng (maximum length) cho trước để đảm bảo tính nhất quán và giảm tải tính toán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d6d0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Các chuỗi sau khi truncation (độ dài tối đa 5) ---\n",
      "Độ dài=5: [3, 11, 10, 1, 5]\n",
      "Độ dài=5: [3, 11, 10, 4, 13]\n",
      "Độ dài=5: [2, 9, 10, 12, 8]\n",
      "\n",
      "--- Các chuỗi sau khi kết hợp Truncation và Padding ---\n",
      "Độ dài=5: [3, 11, 10, 1, 5]\n",
      "Độ dài=5: [3, 11, 10, 4, 13]\n",
      "Độ dài=5: [2, 9, 10, 12, 8]\n",
      "\n",
      "--- Tensor cuối cùng sẵn sàng cho mô hình ---\n",
      "tensor([[ 3, 11, 10,  1,  5],\n",
      "        [ 3, 11, 10,  4, 13],\n",
      "        [ 2,  9, 10, 12,  8]])\n"
     ]
    }
   ],
   "source": [
    "# Giả sử độ dài tối đa cho phép là 5\n",
    "max_len_truncate = 5\n",
    "\n",
    "truncated_sequences = []\n",
    "for seq in sequences:\n",
    "    # Cắt chuỗi nếu nó dài hơn max_len_truncate\n",
    "    truncated_seq = seq[:max_len_truncate]\n",
    "    truncated_sequences.append(truncated_seq)\n",
    "\n",
    "print(f\"--- Các chuỗi sau khi truncation (độ dài tối đa {max_len_truncate}) ---\")\n",
    "for seq in truncated_sequences:\n",
    "    print(f\"Độ dài={len(seq)}: {seq}\")\n",
    "\n",
    "# Lưu ý: Sau khi truncation, các chuỗi vẫn có thể có độ dài khác nhau\n",
    "# Do đó, bước padding thường được thực hiện sau bước truncation.\n",
    "\n",
    "# Kết hợp cả Truncation và Padding\n",
    "final_sequences = []\n",
    "final_length = 5\n",
    "for seq in sequences:\n",
    "    # 1. Cắt bớt\n",
    "    truncated = seq[:final_length]\n",
    "    # 2. Đệm\n",
    "    pad_len = final_length - len(truncated)\n",
    "    padded = truncated + [0] * pad_len\n",
    "    final_sequences.append(padded)\n",
    "    \n",
    "print(\"\\n--- Các chuỗi sau khi kết hợp Truncation và Padding ---\")\n",
    "for seq in final_sequences:\n",
    "    print(f\"Độ dài={len(seq)}: {seq}\")\n",
    "\n",
    "final_tensor = torch.tensor(final_sequences)\n",
    "print(\"\\n--- Tensor cuối cùng sẵn sàng cho mô hình ---\")\n",
    "print(final_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1182fa3f",
   "metadata": {},
   "source": [
    "## **Kết luận**\n",
    "\n",
    "Ta đã tìm hiểu và hiện thực hóa các kỹ thuật cơ bản nhưng vô cùng thiết yếu để chuẩn bị dữ liệu chuỗi cho mô hình RNN. Việc lựa chọn phương pháp mã hóa (Integer, One-Hot, hay Embedding) và cách xử lý độ dài chuỗi (Padding, Truncation) phụ thuộc vào bài toán cụ thể và nguồn tài nguyên tính toán. Trong đó, Word Embedding kết hợp với Padding/Truncation là phương pháp tiêu chuẩn và hiệu quả nhất cho hầu hết các bài toán xử lý ngôn ngữ tự nhiên hiện nay."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
