{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b17089",
   "metadata": {},
   "source": [
    "Báo cáo thực hành lab 4\n",
    "\n",
    "Tên: Lê Huỳnh Cao Dương\n",
    "\n",
    "Đề bài: Sequence representation for RNNs involves converting your sequential data into a numerical format that the model can understand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0449f961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU (MPS).\n",
      "PyTorch version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Detect device: MPS (Apple Silicon) > CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple Silicon GPU (MPS).\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU.\")\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56c2145",
   "metadata": {},
   "source": [
    "Tạo bộ dữ liệu mẫu (toy dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7542da84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 8\n",
      "Sample 0: I love this movie -> label 1\n",
      "Sample last: I did not like this movie at all -> label 0\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"I love this movie\",\n",
    "    \"This film is terrible\",\n",
    "    \"What an amazing experience\",\n",
    "    \"I hate this\",\n",
    "    \"Best movie ever\",\n",
    "    \"Worst film ever\",\n",
    "    \"I enjoyed it a lot\",\n",
    "    \"I did not like this movie at all\"\n",
    "]\n",
    "# Labels: 1 = positive, 0 = negative\n",
    "labels = [1, 0, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "print(\"Number of samples:\", len(texts))\n",
    "print(\"Sample 0:\", texts[0], \"-> label\", labels[0])\n",
    "print(\"Sample last:\", texts[-1], \"-> label\", labels[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae00eebd",
   "metadata": {},
   "source": [
    "Tokenization & basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dc6e6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 | Orig: I love this movie\n",
      "   Tokens: ['i', 'love', 'this', 'movie']\n",
      "01 | Orig: This film is terrible\n",
      "   Tokens: ['this', 'film', 'is', 'terrible']\n",
      "02 | Orig: What an amazing experience\n",
      "   Tokens: ['what', 'an', 'amazing', 'experience']\n",
      "03 | Orig: I hate this\n",
      "   Tokens: ['i', 'hate', 'this']\n",
      "04 | Orig: Best movie ever\n",
      "   Tokens: ['best', 'movie', 'ever']\n",
      "05 | Orig: Worst film ever\n",
      "   Tokens: ['worst', 'film', 'ever']\n",
      "06 | Orig: I enjoyed it a lot\n",
      "   Tokens: ['i', 'enjoyed', 'it', 'a', 'lot']\n",
      "07 | Orig: I did not like this movie at all\n",
      "   Tokens: ['i', 'did', 'not', 'like', 'this', 'movie', 'at', 'all']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"\n",
    "    - Lowercase\n",
    "    - Remove characters that are not letters/numbers/whitespace\n",
    "    - Split on whitespace\n",
    "    \"\"\"\n",
    "    text = text.lower().strip()\n",
    "    # remove punctuation except alphanumeric and whitespace\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Apply to our toy dataset\n",
    "tokenized_texts = [simple_tokenize(t) for t in texts]\n",
    "\n",
    "# Print results for inspection\n",
    "for i, (orig, toks) in enumerate(zip(texts, tokenized_texts)):\n",
    "    print(f\"{i:02d} | Orig: {orig}\")\n",
    "    print(f\"   Tokens: {toks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a6b33",
   "metadata": {},
   "source": [
    "re.sub(r\"[^a-z0-9\\s]\", \"\", text): loại tất cả ký tự không phải chữ a–z, số 0–9 hoặc khoảng trắng — đơn giản và hiệu quả cho ví dụ tiếng Anh.\n",
    "\n",
    ".lower(): chuẩn hoá chữ hoa → chữ thường để tránh phân tách cùng từ khác case.\n",
    "\n",
    "text.split(): split theo whitespace → đơn giản, phù hợp với example tiếng Anh.\n",
    "\n",
    "In từng câu cùng token để bạn dễ kiểm tra lỗi (vd punctuation không được loại, chữ hoa, từ rời...)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618d5f71",
   "metadata": {},
   "source": [
    "Build vocabulary (stoi, itos) & show counts\n",
    "Mục đích:\n",
    "\n",
    "Đếm tần suất token trong tập dữ liệu.\n",
    "\n",
    "Tạo vocab với token đặc biệt <PAD> và <UNK>.\n",
    "\n",
    "Tạo hai dict: stoi (string→index) và itos (index→string).\n",
    "\n",
    "In ra kích thước vocab và vài token đầu/đuôi để bạn kiểm tra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38f3fdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total raw tokens: 34\n",
      "Unique tokens (vocab size without specials): 24\n",
      "Vocab size (with specials): 26\n",
      "\n",
      "Top tokens (token:count):\n",
      "  i: 4\n",
      "  this: 4\n",
      "  movie: 3\n",
      "  ever: 2\n",
      "  film: 2\n",
      "  a: 1\n",
      "  all: 1\n",
      "  amazing: 1\n",
      "  an: 1\n",
      "  at: 1\n",
      "\n",
      "Some vocab entries (first 12):\n",
      "  idx 00 -> <PAD>\n",
      "  idx 01 -> <UNK>\n",
      "  idx 02 -> i\n",
      "  idx 03 -> this\n",
      "  idx 04 -> movie\n",
      "  idx 05 -> ever\n",
      "  idx 06 -> film\n",
      "  idx 07 -> a\n",
      "  idx 08 -> all\n",
      "  idx 09 -> amazing\n",
      "  idx 10 -> an\n",
      "  idx 11 -> at\n",
      "\n",
      "Index of PAD token: 0\n",
      "Index of UNK token: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# tokenized_texts đã có từ Cell 3\n",
    "all_tokens = [tok for sent in tokenized_texts for tok in sent]\n",
    "vocab_counter = Counter(all_tokens)\n",
    "\n",
    "# Special tokens\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# Build vocab: special tokens first, then tokens sorted by frequency desc then alphabetically\n",
    "sorted_tokens = sorted(vocab_counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "vocab = [PAD_TOKEN, UNK_TOKEN] + [tok for tok, _ in sorted_tokens]\n",
    "\n",
    "stoi = {tok: i for i, tok in enumerate(vocab)}\n",
    "itos = {i: tok for tok, i in stoi.items()}\n",
    "\n",
    "# Print summary\n",
    "print(\"Total raw tokens:\", len(all_tokens))\n",
    "print(\"Unique tokens (vocab size without specials):\", len(vocab) - 2)\n",
    "print(\"Vocab size (with specials):\", len(vocab))\n",
    "print()\n",
    "print(\"Top tokens (token:count):\")\n",
    "for tok, cnt in sorted_tokens[:10]:\n",
    "    print(f\"  {tok}: {cnt}\")\n",
    "print()\n",
    "print(\"Some vocab entries (first 12):\")\n",
    "for i in range(min(12, len(vocab))):\n",
    "    print(f\"  idx {i:02d} -> {vocab[i]}\")\n",
    "print()\n",
    "print(\"Index of PAD token:\", stoi[PAD_TOKEN])\n",
    "print(\"Index of UNK token:\", stoi[UNK_TOKEN])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d73229",
   "metadata": {},
   "source": [
    "Text → Integer Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30ac481d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer-encoded sequences:\n",
      "00: [2, 21, 3, 4]\n",
      "01: [3, 6, 17, 23]\n",
      "02: [24, 10, 9, 15]\n",
      "03: [2, 16, 3]\n",
      "04: [12, 4, 5]\n",
      "05: [25, 6, 5]\n",
      "06: [2, 14, 18, 7, 20]\n",
      "07: [2, 13, 22, 19, 3, 4, 11, 8]\n"
     ]
    }
   ],
   "source": [
    "UNK_IDX = stoi[\"<UNK>\"]\n",
    "\n",
    "def text_to_sequence(tokens, stoi):\n",
    "    \"\"\"\n",
    "    Convert list of tokens to list of integer indices.\n",
    "    If a token is not in vocab -> assign UNK index.\n",
    "    \"\"\"\n",
    "    return [stoi.get(tok, UNK_IDX) for tok in tokens]\n",
    "\n",
    "# Apply encoding to all sentences\n",
    "sequences = [text_to_sequence(tokens, stoi) for tokens in tokenized_texts]\n",
    "\n",
    "print(\"Integer-encoded sequences:\")\n",
    "for i, seq in enumerate(sequences):\n",
    "    print(f\"{i:02d}:\", seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a7049f",
   "metadata": {},
   "source": [
    "Padding & Truncation (xử lý độ dài chuỗi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9a4a310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequences:\n",
      "00: [2, 21, 3, 4, 0, 0]  (orig length: 4 )\n",
      "01: [3, 6, 17, 23, 0, 0]  (orig length: 4 )\n",
      "02: [24, 10, 9, 15, 0, 0]  (orig length: 4 )\n",
      "03: [2, 16, 3, 0, 0, 0]  (orig length: 3 )\n",
      "04: [12, 4, 5, 0, 0, 0]  (orig length: 3 )\n",
      "05: [25, 6, 5, 0, 0, 0]  (orig length: 3 )\n",
      "06: [2, 14, 18, 7, 20, 0]  (orig length: 5 )\n",
      "07: [2, 13, 22, 19, 3, 4]  (orig length: 6 )\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "PAD_IDX = stoi[\"<PAD>\"]\n",
    "MAX_LEN = 6  # chosen based on observed sentence lengths\n",
    "\n",
    "def pad_sequences(seqs: List[List[int]], max_len:int, pad_idx:int=PAD_IDX):\n",
    "    padded = []\n",
    "    lengths = []  # store original lengths before padding\n",
    "    for seq in seqs:\n",
    "        if len(seq) >= max_len:\n",
    "            padded.append(seq[:max_len])  # truncation\n",
    "            lengths.append(max_len)\n",
    "        else:\n",
    "            padded.append(seq + [pad_idx] * (max_len - len(seq)))  # padding\n",
    "            lengths.append(len(seq))\n",
    "    return padded, lengths\n",
    "\n",
    "padded_seqs, seq_lengths = pad_sequences(sequences, MAX_LEN, PAD_IDX)\n",
    "\n",
    "print(\"Padded sequences:\")\n",
    "for i, seq in enumerate(padded_seqs):\n",
    "    print(f\"{i:02d}:\", seq, \" (orig length:\", seq_lengths[i], \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c6234c",
   "metadata": {},
   "source": [
    "One-Hot Encoding (demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5329337d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded tensor shape: (8, 6, 26)\n",
      "Example (sentence 0):\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "def one_hot_encode(padded_sequences, vocab_size):\n",
    "    num_sentences = len(padded_sequences)\n",
    "    seq_len = len(padded_sequences[0])\n",
    "    one_hot = np.zeros((num_sentences, seq_len, vocab_size), dtype=np.float32)\n",
    "\n",
    "    for i in range(num_sentences):\n",
    "        for j in range(seq_len):\n",
    "            token_idx = padded_sequences[i][j]\n",
    "            one_hot[i, j, token_idx] = 1.0\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "one_hot_result = one_hot_encode(padded_seqs, vocab_size)\n",
    "\n",
    "print(\"One-hot encoded tensor shape:\", one_hot_result.shape)\n",
    "print(\"Example (sentence 0):\")\n",
    "print(one_hot_result[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f81b7af",
   "metadata": {},
   "source": [
    "Embedding Representation + RNN (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91460f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X shape: torch.Size([2, 6])\n",
      "Batch y shape: torch.Size([2])\n",
      "tensor([[ 2, 21,  3,  4,  0,  0],\n",
      "        [25,  6,  5,  0,  0,  0]])\n",
      "tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, padded_seqs, labels):\n",
    "        self.X = torch.tensor(padded_seqs, dtype=torch.long)\n",
    "        self.y = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset = TextDataset(padded_seqs, labels)\n",
    "\n",
    "# We use small batch size to inspect things clearly\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "for xb, yb in dataloader:\n",
    "    print(\"Batch X shape:\", xb.shape)\n",
    "    print(\"Batch y shape:\", yb.shape)\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ec59e7",
   "metadata": {},
   "source": [
    "Định nghĩa mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "394056ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (embedding): Embedding(26, 32, padding_idx=0)\n",
       "  (lstm): LSTM(32, 64, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1) Embedding layer: học vector cho mỗi token\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # 2) LSTM layer: học quan hệ tuần tự\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 3) Fully-connected layer: phân loại\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len)\n",
    "        \n",
    "        embedded = self.embedding(x)  \n",
    "        # embedded shape: (batch, seq_len, embed_dim)\n",
    "        \n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # hidden shape: (num_layers, batch, hidden_dim)\n",
    "        \n",
    "        # Với 1 layer: lấy hidden state cuối\n",
    "        last_hidden = hidden[-1]  # shape: (batch, hidden_dim)\n",
    "        \n",
    "        logits = self.fc(last_hidden)  # shape: (batch, output_dim)\n",
    "        return logits\n",
    "\n",
    "# Instantiate model\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 32       # có thể tăng lên 50–300 để mô hình mạnh hơn\n",
    "hidden_dim = 64\n",
    "output_dim = 2       # positive / negative\n",
    "pad_idx = stoi[\"<PAD>\"]\n",
    "\n",
    "model = SentimentLSTM(vocab_size, embed_dim, hidden_dim, output_dim, pad_idx).to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36472794",
   "metadata": {},
   "source": [
    "Training Loop\n",
    "\n",
    "Loss function: CrossEntropyLoss\n",
    "\n",
    "Optimizer: Adam\n",
    "\n",
    "Train 30 epoch (vì dataset nhỏ sẽ học nhanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "290384f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Loss: 2.9493 - Acc: 0.3750\n",
      "Epoch 2/30 - Loss: 2.4702 - Acc: 0.6250\n",
      "Epoch 3/30 - Loss: 1.8880 - Acc: 0.8750\n",
      "Epoch 4/30 - Loss: 1.0469 - Acc: 1.0000\n",
      "Epoch 5/30 - Loss: 0.1586 - Acc: 1.0000\n",
      "Epoch 6/30 - Loss: 0.0079 - Acc: 1.0000\n",
      "Epoch 7/30 - Loss: 0.0017 - Acc: 1.0000\n",
      "Epoch 8/30 - Loss: 0.0002 - Acc: 1.0000\n",
      "Epoch 9/30 - Loss: 0.0001 - Acc: 1.0000\n",
      "Epoch 10/30 - Loss: 0.0000 - Acc: 1.0000\n",
      "Epoch 11/30 - Loss: 0.0000 - Acc: 1.0000\n",
      "Epoch 12/30 - Loss: 0.0000 - Acc: 1.0000\n",
      "Epoch 13/30 - Loss: 0.0000 - Acc: 1.0000\n",
      "Epoch 14/30 - Loss: 0.0000 - Acc: 1.0000\n",
      "Epoch 15/30 - Loss: 0.0000 - Acc: 1.0000\n",
      "Epoch 16/30 - Loss: 0.0000 - Acc: 1.0000\n",
      "Epoch 17/30 - Loss: 0.0000 - Acc: 1.0000\n",
      "Epoch 18/30 - Loss: 0.0000 - Acc: 1.0000\n",
      "Epoch 19/30 - Loss: 0.0000 - Acc: 1.0000\n",
      "Epoch 20/30 - Loss: 0.0000 - Acc: 1.0000\n",
      "Epoch 21/30 - Loss: 0.0000 - Acc: 1.0000\n",
      "Epoch 22/30 - Loss: 0.0000 - Acc: 1.0000\n",
      "Epoch 23/30 - Loss: 0.0000 - Acc: 1.0000\n",
      "Epoch 24/30 - Loss: 0.0000 - Acc: 1.0000\n",
      "Epoch 25/30 - Loss: 0.0000 - Acc: 1.0000\n",
      "Epoch 26/30 - Loss: 0.0000 - Acc: 1.0000\n",
      "Epoch 27/30 - Loss: 0.0000 - Acc: 1.0000\n",
      "Epoch 28/30 - Loss: 0.0000 - Acc: 1.0000\n",
      "Epoch 29/30 - Loss: 0.0000 - Acc: 1.0000\n",
      "Epoch 30/30 - Loss: 0.0000 - Acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for xb, yb in dataloader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        \n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    \n",
    "    acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Acc: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6425b1",
   "metadata": {},
   "source": [
    "Dự đoán câu mới (Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e4a24ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1) tokenize\n",
    "    toks = simple_tokenize(sentence)\n",
    "    \n",
    "    # 2) convert to indices\n",
    "    seq = text_to_sequence(toks, stoi)\n",
    "    \n",
    "    # 3) pad\n",
    "    if len(seq) < MAX_LEN:\n",
    "        seq = seq + [PAD_IDX] * (MAX_LEN - len(seq))\n",
    "    else:\n",
    "        seq = seq[:MAX_LEN]\n",
    "        \n",
    "    seq = torch.tensor([seq], dtype=torch.long).to(device)\n",
    "    \n",
    "    # 4) forward\n",
    "    logits = model(seq)\n",
    "    pred = logits.argmax(dim=1).item()\n",
    "    \n",
    "    return \"Positive\" if pred == 1 else \"Negative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2eb2e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "Negative\n",
      "Positive\n",
      "Negative\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(model, \"I really enjoyed this movie\"))\n",
    "print(predict_sentiment(model, \"I hate this film\"))\n",
    "print(predict_sentiment(model, \"not good at all\"))\n",
    "print(predict_sentiment(model, \"this was amazing\"))\n",
    "print(predict_sentiment(model, \"worst movie ever\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e68c049",
   "metadata": {},
   "source": [
    "Mặc dù mô hình đạt 100% accuracy trên tập huấn luyện, nhưng dự đoán trên câu mới còn sai → chứng tỏ hiện tượng overfitting do tập dữ liệu nhỏ.\n",
    "Điều này cho thấy vai trò quan trọng của kích thước dữ liệu và biểu diễn embedding trong bài toán NLP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL_Practice venv)",
   "language": "python",
   "name": "dl_practice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
